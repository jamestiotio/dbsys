{
  "metadata": {
    "name": "Exercise2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Exercise 2\n\nIn this exercise we use PySpark to build a binary classifier to classify a given tweet is about KPOP or other topics using a supervised machine learning technique, SVM.\n\nFor parts marked with **[CODE CHANGE REQUIRED]** you need to modify or complete the code before execution.\nFor parts without **[CODE CHANGE REQUIRED]** , you can just run the given code.\n\nThe task here is to build a classifier to differentiate the KPOP tweets or otherwise.\n\nFor example, the following tweet message falls into the category of Korean Pop because it seems talking about someone from korea \n```text\ncrazy cool jae s lee\u0027s pic of street singer reflected in raindrops tuesday on 2nd ave  \n```\nOn the other hand, the following tweet is not revelant to KPOP. \n```text\naccident closes jae valley rd drivers advised to avoid area seek alternate routes\n```\nTo achieve the goal, we need to develop a classifier, which is a supervised machine learning technique. In this example, we consider using Support Vector Machine (SVM) as the classifier algorithm. On the higher level, we need to \"train\" the model with some manually labelled data and perform some tests against the trained model. As part of the input requirement the SVM expect the input data to represented as a label (either yes or no, 1 or 0) accompanied by the feature vector. The feature vector is a vector of values which uniquely differentiate one entry from another ideally. In the machine learning context, features have to be fixed by the programmers. \n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Uploading the data\n\n**[CODE CHANGE REQUIRED]** \nModify the following bash cell according to your environment and upload the data.\n\nIn case running the below taking too long thus Zeppelin killed it. e.g. \n\n```text\nParagraph received a SIGTERM\nExitValue: 143\n```\n\nYou may copy, paste and run the commands in a terminal (via ssh).\n\nHowever due to a bug with hadoop version 3.3.x, we still see the following warning, which is fine.\n\n```text\n2021-11-03 14:39:57,306 WARN hdfs.DataStreamer: Caught exception\njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1252)\n\tat java.lang.Thread.join(Thread.java:1326)\n\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n```"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nexport PATH\u003d$PATH:/home/ec2-user/hadoop/bin/\n\nnamenode\u003dip-172-31-86-18 # TODO:change me\n\nhdfs dfs -rm -r hdfs://$namenode:9000/lab12/ex2/\nhdfs dfs -mkdir -p hdfs://$namenode:9000/lab12/ex2/\nhdfs dfs -put /home/ec2-user/git/50043-labs/lab12/data/ex2/label_data hdfs://$namenode:9000/lab12/ex2/\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Importing and Setup\n\n**[CODE CHANGE REQUIRED]**\n\nLet\u0027s import all the require libraries and set the hadoop file system name node IP.\n\nWe make use of `numpy` a python library for numeric computation,\nIf Python complains about `numpy not found`, go to terminal and run in all the data nodes that you have in the cluster\n\n```bash\n$ sudo pip3 install numpy sets\n```\n\nAlternatively, you may can also use flintrock to issue the above command to all the nodes in your cluster\n\n```bash\n$ flintrock run-command my_test_cluster \u0027sudo pip3 install sets numpy\u0027\n```"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nimport re\nimport sets, math\nimport numpy # make sure numpy is installed on all datanodes using the command pip3 install numpy\n\nfrom pyspark.sql import SQLContext\nfrom pyspark.mllib import *\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.classification import SVMWithSGD\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\nsparkSession \u003d SparkSession.builder.appName(\"SVM notebook\").getOrCreate()\nsc \u003d sparkSession.sparkContext\n\nhdfs_nn \u003d \"ip-172-31-86-18\" # TODO: fixme\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Loading the data\nWe load the data from the HDFS. The `.sample(False,0.1)` is to perform sampling on the input dataset. If you are to run it on a full cluster, feel free to remove the sampling\n\nThe first argument is boolean flag is called `withReplacement`. When it is `True`, it allows the same element to appear more than once. \nThe second argument is the fraction of elements in the sampled results. `0.1` means we expect 10% of the entire data set in the samples. You might set it to a lower ratio if it takes too long to run in t2.micro.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef remove_punct(tweet):\n    return re.sub(\u0027[\\\u0027\".,!#]\u0027,\u0027\u0027,tweet)\n\nposTXT \u003d sc.textFile(\"hdfs://%s:9000/lab12/ex2/label_data/Kpop/*.txt\" % hdfs_nn).sample(False,0.1).map(remove_punct)\nnegTXT \u003d sc.textFile(\"hdfs://%s:9000/lab12/ex2/label_data/othertweet/*.txt\" % hdfs_nn).sample(False,0.1).map(remove_punct)\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## Exercise 2.1 Build a language model using TFIDF\n\nIn Natural Language Procoessing, we often model a language using bags of words model. The idea is to represent text in terms of vectors.\n\nOne of the simple and effective method is to use Term-Frequency Inversed Document Frequency.\n\n$$\nTFIDF(w) \u003d TF(w) * log(NDoc/DF(w))\n$$\n\nwhere *NDoc* is the number of documents.\n\n\n*  TF is actually the word count. For instance, consider the following text data.\n```text\napple smart phones made by apple\nandroid smart phones made by others\n```\nWe assume that each line is a document, hence there are two documents here.\n\n* The term frequency is\n```text\napple, 2\nandroid, 1\nphones, 2\nsmart, 2\nmade, 2\nby, 2\nothers, 1\n```\nThe term frequency is basically the word count, i.e. the number of occurances of a word across all document.\n\n* The document frequency is \n\n```text\napple, 1\nandroid, 1\nphones, 2\nsmart, 2\nmade, 2\nby, 2\nothers, 1\n```\n\nThe document frequency is the number of documents a word is mentioned.\n\n\n* IDF is is the total number of documents/records divided by the total number of the documents/records containing the words. We apply logarithmic to the quotient. The IDF for the above example is\n```text\napple, log(2/1)\nandroid, log(2/1)\nphones, log(2/2)\nsmart, log(2/2)\nmade, log(2/2)\nby, log(2/2)\nothers, log(2/1)\n```\nthat is\n```text\napple, 0.693\nandroid, 0.693\nphones, 0\nsmart, 0\nmade, 0\nby, 0\nothers, 0.693\n```\n\n* TF-IDF is obtained by multiplying the TF with the IDF.\n```text\napple, 1.386\nandroid, 0.693\nphones, 0\nsmart, 0\nmade, 0\nby, 0\nothers, 0.693\n```\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Define `tf`\n**[CODE CHANGE REQUIRED]** \nComplete the following snippet to define `tf`\n\n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\ntf is the same as word count\n```"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef tf(terms): \n    \u0027\u0027\u0027\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, tf_score)\n    \u0027\u0027\u0027\n    # TODO\n    return None\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Sample answer \n\n\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```python\ndef tf(terms): \n    \u0027\u0027\u0027\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, tf_score)\n    \u0027\u0027\u0027\n    # ANSWER\n    return terms.flatMap(lambda seq: map(lambda w:(w,1), seq)).reduceByKey(lambda x,y:x + y)\n\n```\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Test Case for `tf`\n\nRun the following cell, you should see\n\n```\n[(\u0027apple\u0027, 2), (\u0027by\u0027, 2), (\u0027android\u0027, 1), (\u0027smart\u0027, 2), (\u0027made\u0027, 2), (\u0027phones\u0027, 2), (\u0027others\u0027, 1)]\n```\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\ndef one_grams(s):\n    return s.split()\n\n\ntest_terms \u003d [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\ntest_tf \u003d tf(sc.parallelize(test_terms))\ntest_tf.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Define `df`\n\n**[CODE CHANGE REQUIRED]** \n\nComplete the following snippet to define `df`\n\n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\ndf differs from tf with a little bit. Instead of outputting (word,1) for every word in a tweet directly, we should remove the duplicating words (within the same tweet) first.\n```"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef df(terms): \n    \u0027\u0027\u0027\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, df_score)\n    \u0027\u0027\u0027\n    # TODO\n    return None\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Test Case for `df`\n\nRun the following cell, you will see\n\n```\n[(\u0027apple\u0027, 1), (\u0027by\u0027, 2), (\u0027android\u0027, 1), (\u0027smart\u0027, 2), (\u0027made\u0027, 2), (\u0027phones\u0027, 2), (\u0027others\u0027, 1)]\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntest_terms \u003d [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\ntest_df \u003d df(sc.parallelize(test_terms))\ntest_df.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Sample answer\n\n\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```python\ndef df(terms): \n    \u0027\u0027\u0027\n    input\n    terms :  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (word, df_score)\n    \u0027\u0027\u0027\n    # ANSWER\n    return terms.flatMap(lambda seq: list(set(map(lambda w:(w,1), seq)))).reduceByKey(lambda x,y:x + y)\n```\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Define `tfidf`\n\n**[CODE CHANGE REQUIRED]** \n\nComplete the following snippet to define `tfidf`\n\n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\nLet r be an RDD. r.count() returns the size of r.\nLet r1, r2 be RDDs of key-value pairs. r1.join(r2) joins two RDDs by keys.\n```"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef tfidf(terms): \n    \u0027\u0027\u0027\n    input\n    terms:  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (words, tfidf_score) sorted by tfidf_score in descending order.\n    \u0027\u0027\u0027\n    # TODO\n    return None\n    \n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Sample answer\n\n\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```python\ndef tfidf(terms): \n    \u0027\u0027\u0027\n    input\n    terms:  a RDD of lists of terms (words)\n    output\n    a RDD of pairs i.e. (words, tfidf_score) sorted by tfidf_score in descending order.\n    \u0027\u0027\u0027\n    # ANSWER\n    dCount \u003d terms.count()\n    tfreq \u003d tf(terms)\n    dfreq \u003d df(terms)\n    return tfreq.join(dfreq).map(lambda p :(p[0], p[1][0] * math.log(dCount/p[1][1]))).sortBy( lambda p : - p[1])\n```\n\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Test case for `tfidf`\n\nRun the following cell you will see\n\n```\n[(\u0027apple\u0027, 1.3862943611198906), (\u0027android\u0027, 0.6931471805599453), (\u0027others\u0027, 0.6931471805599453), (\u0027by\u0027, 0.0), (\u0027smart\u0027, 0.0), (\u0027made\u0027, 0.0), (\u0027phones\u0027, 0.0)]\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntest_terms \u003d [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\ntest_tfidf \u003d tfidf(sc.parallelize(test_terms))\ntest_tfidf.collect()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercise 2.2 Defining the Label points\n\nRecall that each label point is a decimal value (the label) with a vector. \n\n* For all positive tweets (KPop tweets) the label will be `1` and for all negative tweets we set `0` as the label. \n* For the vector parts, we build them using the tweet messages and the top 150 TFIDF\n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# You don\u0027t need to modify this cell\ndef buildTopTFIDF(tweets,tokenizer):\n    \u0027\u0027\u0027\n    input\n    tweets: an RDD of texts|\n    tokenizer: a function turns a string into list of tokens\n    \n    output\n    a list containing top 150 tfidf terms\n    \u0027\u0027\u0027\n    terms \u003d tweets.map(tokenizer)\n    return map(lambda p:p[0], tfidf(terms).take(150))\n    "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Tokenizer\n**[CODE CHANGE REQUIRED]** \nWe\u0027ve been using single word tokens for the test cases. However sometime using a multi-word tokenizer will help improving the performance by taking the neighboring word into account. \nDefine a `two_grams` tokenizer\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom functools import reduce\n\ndef two_grams(str):\n   \u0027\u0027\u0027\n    input\n     str : a string\n    output\n     a list of strings (each string contains two consecutive words seperated by space)\n   \u0027\u0027\u0027\n   return None # TODO: fixme \n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Sample answer\n\n\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```python\ndef to_ngrams(str, n):\n    words \u003d str.split()\n    tokens \u003d [words] * (n-1) # replicate the list of words for n-1 times\n    dropped \u003d map(lambda p: p[0][p[1]:], zip(tokens, range(1,n)))\n    return reduce(lambda acc,ts:map(lambda p : p[0] + \" \" + p[1], zip(acc,ts)), dropped, words)\n\ndef two_grams(str):\n    return to_ngrams(str, 2)\n\n\n```\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Test Case for `two_grams`\n\nRun the following you should see \n\n```text\n[\u0027The virus\u0027, \u0027virus that\u0027, \u0027that causes\u0027, \u0027causes COVID-19\u0027, \u0027COVID-19 is\u0027, \u0027is mainly\u0027, \u0027mainly transmitted\u0027, \u0027transmitted through\u0027, \u0027through droplets\u0027]\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ns \u003d \"The virus that causes COVID-19 is mainly  transmitted through droplets\"\nlist(two_grams(s))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The following cells build the top 150 TFIDF from the data that we loaded, you don\u0027t need to change anything. It might take a while to run (~ 25 mins on my t2.micro cluster)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntopTFIDF \u003d  buildTopTFIDF(posTXT + negTXT,two_grams)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntype(topTFIDF)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n## Defining `computeLP`\n**[CODE CHANGE REQUIRED]** \nComplete the following snippet.\n\nConcretely speaking, the `computeLP` function takes a label `1.0` or `0.0`, a sequence of string i.e. the 2-grams or 3-grams, and a array of top-N TF-IDF.\n\nFor each tf-idf term, let\u0027s say `t` is the i-th top-N TF-IDF term, if `t` is in the sequence of strings, we should put a `1.0` at the i-th position of the output vector, otherwise it should be `0.0`.\n\n\u003cstyle\u003e\n    div.hidecode + pre {display: none}\n\u003c/style\u003e\n\u003cscript\u003e\ndoclick\u003dfunction(e) {\n    e.nextSibling.nextSibling.style.display\u003d\"block\";\n}\n\u003c/script\u003e\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n```text\nConvert all the words in the input text into a set instead of a list.\nThe output vector should be of the same dimension as topTerms (AKA top 150 TFIDF).\n```"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\ndef computeLP(label,text,tokenizer,topTerms):\n    \u0027\u0027\u0027\n    input\n    label : label 1 or 0\n    text : the text (String type)\n    tokenizer : the tokenizer\n    topTerms: the top TFIDF terms\n    \n    output:\n    a label point.\n    \u0027\u0027\u0027\n    seqSet \u003d set(tokenizer(text))\n    scores \u003d [0.0] * 150 # TODO: fixme\n    return LabeledPoint(label, Vectors.dense(scores))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Sample answer\n\n\n\u003cdiv class\u003d\"hidecode\" onclick\u003d\"doclick(this);\"\u003e[Show Hint]\u003c/div\u003e\n\n\n```python\ndef computeLP(label,text,tokenizer,topTerms):\n    \u0027\u0027\u0027\n    input\n    label : label 1 or 0\n    text : the text (String type)\n    tokenizer : the tokenizer\n    topTerms: the top TFIDF terms\n    \n    output:\n    a label point.\n    \u0027\u0027\u0027\n    seqSet \u003d set(tokenizer(text))\n    # ANSWER\n    scores \u003d map(lambda t: 1.0 if t in seqSet else 0.0, list(topTerms))\n    return LabeledPoint(label, Vectors.dense(scores))\n````"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Test Case for `computeLP`\n\nRun the following cell, you should see\n\n```\nLabeledPoint(1.0, [0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n```\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncomputeLP(1.0, \"I love yoo jae suk\", two_grams, topTFIDF)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Training the model\n\nLet\u0027s train our model. The codes are written for you, you don\u0027t need to change anything\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nposLP \u003d posTXT.map( lambda twt: computeLP(1.0, twt, two_grams, topTFIDF) )\nnegLP \u003d negTXT.map( lambda twt: computeLP(0.0, twt, two_grams, topTFIDF) )\n\ndata \u003d negLP + posLP\n\n\n# Split data into training (60%) and test (40%).\n\nsplits \u003d data.randomSplit([0.6,0.4],seed \u003d 11L)\ntraining \u003d splits[0].cache()\ntest \u003d splits[1]\n\n# Run training algorithm to build the model\nnum_iteration \u003d 100\nmodel \u003d SVMWithSGD.train(training,num_iteration)\n\n# This will takes about 20 mins on a 4-core intel i7 processor 3.8GHZ with hyperthreading\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercise 2.3 Evaluating the model\n\nWe apply the trained model to our testing data and evaluate the performance of our model. It should be around 84% accurate.\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark \nmodel.clearThreshold()\n# Compute raw scores on the test set\nscore_and_labels \u003d test.map( lambda point: (float(model.predict(point.features)), point.label) )\n\n\n# Get the evaluation metrics\nmetrics \u003d BinaryClassificationMetrics(score_and_labels)\nau_roc \u003d metrics.areaUnderROC\n\nprint(\"Area under ROC \u003d %s\" % str(au_roc))"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsc.stop()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Cleaning up\n**[CODE CHANGE REQUIRED]** \nModify the following to clean up the HDFS\n"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nexport PATH\u003d$PATH:/home/ec2-user/hadoop/bin/\n\nnamenode\u003dip-172-31-86-18 # TODO:change me\n\nhdfs dfs -rm -r hdfs://$namenode:9000/lab12/ex2/\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# End of Exercise 2\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ""
    }
  ]
}